{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "b5aTUG2w_OL4",
        "n4Q-2DOr190-",
        "_XDtoFs52KvA",
        "unr5b0skwWQp"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Package Importer"
      ],
      "metadata": {
        "id": "b5aTUG2w_OL4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HhF4IJknlQvJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests numpy matplotlib tqdm beautifulsoup4 Pillow\n",
        "\n",
        "import os\n",
        "import requests\n",
        "import json\n",
        "from tqdm.notebook import tqdm\n",
        "from torchvision import transforms\n",
        "import base64\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import torch\n",
        "import zipfile\n",
        "from io import BytesIO\n",
        "from tqdm import tqdm\n",
        "from bs4 import BeautifulSoup\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from google.colab import drive, files\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "A98rd2bh_AZ7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96c06dd2-aa6d-422b-883d-d14391946ea4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Using device: cpu\n",
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ty6b1tuYlInn"
      },
      "source": [
        "# **Data Loading Functions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGKp_KWt131Q"
      },
      "source": [
        "## 1. Indus Script Acquisition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZM52IEnrEdzf"
      },
      "source": [
        "Automates the retrieval and processing of Indus script sign data  \n",
        "- Downloads an HTML dataset from a GitHub repository and parses it using BeautifulSoup.  \n",
        "- Extracts sign codes and corresponding images from a structured table.  \n",
        "- Filters entries based on occurrence frequency (threshold > 2) to retain relevant samples.  \n",
        "- Authenticates and downloads images from a restricted-access database.  \n",
        "- Saves the processed dataset and logs key metadata for verification.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "58113c63303a4aa0a55e2c7a89ae2054",
            "a311bfbd68264332b46d3ab5e8920779",
            "c69af49ba0144dddbc309af7b896be25",
            "f6b19c9479194638b9e470008cf1bf85",
            "cde30bf9c78245c5874fa7b3bfc40d52",
            "5090fae773e14f12aa340f4dbf1c0c46",
            "b91f3db95a2340ad946386f5641bf998",
            "b5ae2a22fb8c4c43b04c46502de46cf6",
            "89451ac49d3c4bb1bd3cbeb5d5281602",
            "8e4baf7ed907456cb3c78277f3db6ef2",
            "539c5e742f5a47dbbf065f3377ec5e28"
          ]
        },
        "id": "Kn6EptvJ164K",
        "outputId": "e4f7871b-e271-4761-dbe0-c3ca09b72511"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created directory: /content/drive/MyDrive/script_analysis/indus\n",
            "Fetching HTML file from: https://raw.githubusercontent.com/oohalakkadi/signaturework/main/data/indus/ICIT.html\n",
            "Downloaded HTML file to: /content/drive/MyDrive/script_analysis/indus/ICIT.html\n",
            "Parsing HTML file...\n",
            "Found 715 rows in the table\n",
            "Found 715 total signs\n",
            "Downloading 715 images...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0/715 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "58113c63303a4aa0a55e2c7a89ae2054"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download complete! All images are saved in: /content/drive/MyDrive/script_analysis/indus\n",
            "Downloaded 715 image files\n",
            "Sample files: 1.jpg, 2.jpg, 3.jpg, 4.jpg, 5.jpg... and 710 more\n"
          ]
        }
      ],
      "source": [
        "def ensure_dir(directory):\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "        print(f\"Created directory: {directory}\")\n",
        "    else:\n",
        "        print(f\"Directory already exists: {directory}\")\n",
        "\n",
        "output_dir = \"/content/drive/MyDrive/script_analysis/indus\"\n",
        "ensure_dir(output_dir)\n",
        "\n",
        "repo_owner = \"oohalakkadi\"\n",
        "repo_name = \"ivc2tyc\"\n",
        "branch = \"main\"\n",
        "file_path = \"reference/indus/ICIT.html\"\n",
        "\n",
        "raw_url = f\"https://raw.githubusercontent.com/{repo_owner}/{repo_name}/{branch}/{file_path}\"\n",
        "print(f\"Fetching HTML file from: {raw_url}\")\n",
        "\n",
        "response = requests.get(raw_url)\n",
        "if response.status_code != 200:\n",
        "    print(f\"Failed to fetch HTML file. Status code: {response.status_code}\")\n",
        "    print(f\"Response: {response.text}\")\n",
        "    raise Exception(\"Could not download HTML file from GitHub\")\n",
        "\n",
        "local_html_path = os.path.join(output_dir, \"ICIT.html\")\n",
        "with open(local_html_path, \"wb\") as file:\n",
        "    file.write(response.content)\n",
        "print(f\"Downloaded HTML file to: {local_html_path}\")\n",
        "\n",
        "base_url = \"https://www.indus.epigraphica.de/\"\n",
        "\n",
        "# Authentication details for the Indus website\n",
        "AUTH_USERNAME = \"icit\"\n",
        "AUTH_PASSWORD = \"seal123\"\n",
        "AUTH_HEADER = {\n",
        "    \"Authorization\": \"Basic \" + base64.b64encode(f\"{AUTH_USERNAME}:{AUTH_PASSWORD}\".encode()).decode()\n",
        "}\n",
        "\n",
        "print(\"Parsing HTML file...\")\n",
        "with open(local_html_path, \"r\", encoding=\"utf-8\") as file:\n",
        "    soup = BeautifulSoup(file, \"html.parser\")\n",
        "\n",
        "rows = soup.find_all(\"tr\")[1:]\n",
        "print(f\"Found {len(rows)} rows in the table\")\n",
        "\n",
        "sign_data = []\n",
        "for row in rows:\n",
        "    cols = row.find_all(\"td\")\n",
        "\n",
        "    if len(cols) < 12:\n",
        "        continue\n",
        "\n",
        "    sign_code = cols[0].text.strip()\n",
        "\n",
        "    img_tag = cols[6].find(\"img\")\n",
        "    if img_tag:\n",
        "        img_src = img_tag[\"src\"]\n",
        "        img_url = base_url + img_src\n",
        "\n",
        "        # Include all signs regardless of frequency\n",
        "        sign_data.append((sign_code, img_url))\n",
        "\n",
        "print(f\"Found {len(sign_data)} total signs\")\n",
        "\n",
        "print(f\"Downloading {len(sign_data)} images...\")\n",
        "\n",
        "session = requests.Session()\n",
        "session.headers.update(AUTH_HEADER)\n",
        "\n",
        "for sign_code, img_url in tqdm(sign_data, desc=\"Downloading\"):\n",
        "    img_filename = os.path.join(output_dir, f\"{sign_code}.jpg\")\n",
        "\n",
        "    response = session.get(img_url, stream=True)\n",
        "    if response.status_code == 200:\n",
        "        with open(img_filename, \"wb\") as img_file:\n",
        "            img_file.write(response.content)\n",
        "    else:\n",
        "        print(f\"Failed to download: {img_url} - Status {response.status_code}\")\n",
        "\n",
        "print(\"Download complete! All images are saved in:\", output_dir)\n",
        "\n",
        "downloaded_files = [f for f in os.listdir(output_dir) if f.endswith('.jpg')]\n",
        "print(f\"Downloaded {len(downloaded_files)} image files\")\n",
        "if downloaded_files:\n",
        "    print(f\"Sample files: {', '.join(downloaded_files[:5])}\" +\n",
        "          (f\"... and {len(downloaded_files)-5} more\" if len(downloaded_files) > 5 else \"\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4Q-2DOr190-"
      },
      "source": [
        "## 2. Proto-Cuneiform Script Acquisition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHStHa2MEqpr"
      },
      "source": [
        "\n",
        "\n",
        "Retrieval of the standardized Proto-Cuneiform sign corpus from CDLI GitHub repository.\n",
        "\n",
        "- **Repository**: cdli-gh/proto-cuneiform_signs\n",
        "- **Operation**: Clone repository and extract JPG images from archsigns directory\n",
        "- **Destination**: Google Drive at /content/drive/MyDrive/script_analysis/proto_cuneiform/\n",
        "\n",
        "Establishes the Proto-Cuneiform dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "fbT_25YS2Cgk"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/cdli-gh/proto-cuneiform_signs.git\n",
        "!mkdir -p /content/drive/MyDrive/script_analysis/proto_cuneiform/\n",
        "!cp proto-cuneiform_signs/archsigns/*.jpg /content/drive/MyDrive/script_analysis/proto_cuneiform/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XDtoFs52KvA"
      },
      "source": [
        "## 3. Proto-Elamite Script Acquisition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwJ0X9JPFy2t"
      },
      "source": [
        "Retrieval of Proto-Elamite sign corpus from SFU Natural Language Lab repository.\n",
        "\n",
        "- **Repository**: sfu-natlang/pe-decipher-toolkit\n",
        "- **Operation**: Clone repository and extract PNG images from both main forms and numerical signs, sourced from CDLI\n",
        "- **Sources**: PE_mainforms directory (standard signs) and PE_num directory (numerical notation)\n",
        "- **Destination**: Google Drive at /content/drive/MyDrive/script_analysis/proto_elamite/\n",
        "\n",
        "Establishes the Proto-Elamite dataset containing both standard and numerical signs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "AZzs0aMy2M7J"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/sfu-natlang/pe-decipher-toolkit.git\n",
        "!mkdir -p /content/drive/MyDrive/script_analysis/proto_elamite/\n",
        "!cp pe-decipher-toolkit/pngs/PE_mainforms/*.png /content/drive/MyDrive/script_analysis/proto_elamite/\n",
        "!cp pe-decipher-toolkit/pngs/PE_num/*.png /content/drive/MyDrive/script_analysis/proto_elamite/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lk-7mzp62cpd"
      },
      "source": [
        "## 4. Naxi Dongba Script Acquisition and Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uBIZXbDHe95"
      },
      "source": [
        "\n",
        "Implementation of font-based character rendering pipeline for Naxi Dongba script dataset creation.\n",
        "\n",
        "- **Source**: BabelStoneNaxiLLC.ttf font file containing standardized Dongba glyphs\n",
        "- **Unicode Range**: U+E000 to U+E849 (2,122 Dongba characters in Private Use Area)\n",
        "- **Rendering Process**:\n",
        "  - Character-by-character rendering with centered positioning\n",
        "  - Consistent 224×224px dimensions for neural network compatibility\n",
        "  - PNG format with lossless compression\n",
        "- **Visual Verification**: Sample character display with Unicode code point labeling\n",
        "- **Naming Convention**: Unicode-based filenames (dongba_{hex_code}.png)\n",
        "\n",
        "Creates programmatically generated Naxi Dongba glyph dataset with standardized rendering parameters for script comparison analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "i-5empIa2ebi"
      },
      "outputs": [],
      "source": [
        "def ensure_dir(directory):\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "        print(f\"Created directory: {directory}\")\n",
        "    else:\n",
        "        print(f\"Directory already exists: {directory}\")\n",
        "\n",
        "def download_font_from_github(output_path):\n",
        "    \"\"\"Download the TTF font file from GitHub repository\"\"\"\n",
        "\n",
        "    repo_owner = \"oohalakkadi\"\n",
        "    repo_name = \"ivc2tyc\"\n",
        "    branch = \"main\"\n",
        "    font_path = \"reference/tyc/naxi/BabelStoneNaxiLLC.ttf\"\n",
        "\n",
        "\n",
        "    raw_url = f\"https://raw.githubusercontent.com/{repo_owner}/{repo_name}/{branch}/{font_path}\"\n",
        "    print(f\"Downloading font from: {raw_url}\")\n",
        "\n",
        "\n",
        "    response = requests.get(raw_url)\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Failed to fetch font file. Status code: {response.status_code}\")\n",
        "        raise Exception(\"Could not download font file from GitHub\")\n",
        "\n",
        "\n",
        "    with open(output_path, \"wb\") as file:\n",
        "        file.write(response.content)\n",
        "    print(f\"Downloaded font to: {output_path}\")\n",
        "    return output_path\n",
        "\n",
        "def char_to_image(char, font, image_size=224, bg_color=(255, 255, 255), text_color=(0, 0, 0)):\n",
        "    img = Image.new('RGB', (image_size, image_size), color=bg_color)\n",
        "    draw = ImageDraw.Draw(img)\n",
        "\n",
        "    left, top, right, bottom = font.getbbox(char)\n",
        "    text_width = right - left\n",
        "    text_height = bottom - top\n",
        "\n",
        "    # center text\n",
        "    position = ((image_size - text_width) // 2 - left, (image_size - text_height) // 2 - top)\n",
        "\n",
        "    draw.text(position, char, fill=text_color, font=font)\n",
        "\n",
        "    return img\n",
        "\n",
        "\n",
        "def create_dongba_dataset(font_path, output_dir, start_code=0xE000, end_code=0xE849, image_size=224):\n",
        "    ensure_dir(output_dir)\n",
        "\n",
        "    font_size = int(image_size * 0.7)\n",
        "    try:\n",
        "        font = ImageFont.truetype(font_path, font_size)\n",
        "        print(f\"Font loaded successfully: {font_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading font {font_path}: {e}\")\n",
        "        return\n",
        "\n",
        "    char_list = [chr(code) for code in range(start_code, end_code + 1)]\n",
        "    print(f\"Will generate images for {len(char_list)} characters (U+{start_code:04X} to U+{end_code:04X})\")\n",
        "\n",
        "\n",
        "    font_name = os.path.splitext(os.path.basename(font_path))[0]\n",
        "\n",
        "\n",
        "    plt.figure(figsize=(15, 3))\n",
        "    sample_indices = np.linspace(0, len(char_list)-1, 5, dtype=int)\n",
        "\n",
        "    for i, idx in enumerate(sample_indices):\n",
        "        char = char_list[idx]\n",
        "        img = char_to_image(char, font, image_size=image_size)\n",
        "        plt.subplot(1, 5, i+1)\n",
        "        plt.imshow(np.array(img))\n",
        "        plt.title(f\"U+{ord(char):04X}\")\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    for i, char in tqdm(enumerate(char_list), total=len(char_list), desc=\"Generating images\"):\n",
        "        try:\n",
        "\n",
        "            char_code = f\"{ord(char):04X}\"\n",
        "            filename = f\"dongba_{char_code}.png\"\n",
        "\n",
        "\n",
        "            img = char_to_image(char, font, image_size=image_size)\n",
        "\n",
        "\n",
        "            img.save(os.path.join(output_dir, filename))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing character U+{ord(char):04X}: {e}\")\n",
        "\n",
        "    print(f\"Created {len(char_list)} character images in {output_dir}\")\n",
        "    return output_dir\n",
        "\n",
        "\n",
        "output_dir = '/content/drive/MyDrive/script_analysis/naxi_dongba/'\n",
        "ensure_dir(output_dir)\n",
        "\n",
        "\n",
        "temp_font_path = '/content/BabelStoneNaxiLLC.ttf'\n",
        "\n",
        "\n",
        "downloaded_font_path = download_font_from_github(temp_font_path)\n",
        "\n",
        "\n",
        "create_dongba_dataset(\n",
        "    font_path=downloaded_font_path,\n",
        "    output_dir=output_dir,\n",
        "    start_code=0xE000,\n",
        "    end_code=0xE849,\n",
        "    image_size=224\n",
        ")\n",
        "\n",
        "# Count generated files as verification\n",
        "generated_files = [f for f in os.listdir(output_dir) if f.startswith('dongba_') and f.endswith('.png')]\n",
        "print(f\"\\nGeneration complete! {len(generated_files)} images saved to: {output_dir}\")\n",
        "\n",
        "# Force sync to Google Drive\n",
        "print(\"Syncing files to Google Drive...\")\n",
        "try:\n",
        "    with open('/content/drive/MyDrive/__force_sync__', 'w') as f:\n",
        "        f.write('')\n",
        "    os.remove('/content/drive/MyDrive/__force_sync__')\n",
        "except:\n",
        "    pass  # Ignore errors with the sync file\n",
        "print(\"Sync complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "QQmH_lOwQw1Z"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unr5b0skwWQp"
      },
      "source": [
        "## 5. Old Naxi (Dongba) Script Acquisition\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code is in **old_naxi.ipynb**."
      ],
      "metadata": {
        "id": "CauhaJK6QSdx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhKYFG4EQjQ5"
      },
      "source": [
        "## 6. TYC Scripts Acquisition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aibf1rn9Uu5v"
      },
      "source": [
        "### Ba-Shu Dataset Acquisition (symbols transcribed from Sanxingdui Museum)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fetches and stores Ba-Shu script dataset from GitHub to Google Drive.  "
      ],
      "metadata": {
        "id": "1XaH8koNQvP4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2qNmKaESQ8_"
      },
      "outputs": [],
      "source": [
        "def ensure_dir(directory):\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "    print(f\"Directory ready: {directory}\")\n",
        "\n",
        "drive_bashu_dir = '/content/drive/MyDrive/script_analysis/ba-shu/'\n",
        "ensure_dir(drive_bashu_dir)\n",
        "\n",
        "repo_owner = \"oohalakkadi\"\n",
        "repo_name = \"ivc2tyc\"\n",
        "branch = \"main\"\n",
        "folder_path = \"reference/tyc/ba-shu\"\n",
        "\n",
        "api_url = f\"https://api.github.com/repos/{repo_owner}/{repo_name}/contents/{folder_path}?ref={branch}\"\n",
        "print(f\"Fetching file list from: {api_url}\")\n",
        "\n",
        "response = requests.get(api_url)\n",
        "if response.status_code != 200:\n",
        "    print(f\"Failed to fetch repository contents. Status code: {response.status_code}\")\n",
        "    print(f\"Response: {response.text}\")\n",
        "else:\n",
        "    files = json.loads(response.text)\n",
        "    print(f\"Found {len(files)} files in the repository folder\")\n",
        "\n",
        "    for file in files:\n",
        "        if file['type'] == 'file':\n",
        "            file_url = file['download_url']\n",
        "            file_name = file['name']\n",
        "\n",
        "            print(f\"Downloading: {file_name}\")\n",
        "\n",
        "            file_response = requests.get(file_url)\n",
        "            if file_response.status_code == 200:\n",
        "\n",
        "                destination_path = os.path.join(drive_bashu_dir, file_name)\n",
        "                with open(destination_path, 'wb') as f:\n",
        "                    f.write(file_response.content)\n",
        "                print(f\"Saved: {file_name}\")\n",
        "            else:\n",
        "                print(f\"Failed to download {file_name}. Status code: {file_response.status_code}\")\n",
        "\n",
        "    print(\"Syncing files to Google Drive...\")\n",
        "    with open('/content/drive/MyDrive/__force_sync__', 'w') as f:\n",
        "        f.write('')\n",
        "    os.remove('/content/drive/MyDrive/__force_sync__')\n",
        "\n",
        "    downloaded_files = os.listdir(drive_bashu_dir)\n",
        "    print(f\"\\nDownload complete! {len(downloaded_files)} files saved in: {drive_bashu_dir}\")\n",
        "    print(f\"Files: {', '.join(downloaded_files[:5])}\" +\n",
        "          (f\"... and {len(downloaded_files)-5} more\" if len(downloaded_files) > 5 else \"\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piRBiY5hXGth"
      },
      "source": [
        "### Classical Yi Data Acquisition"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloads, extracts, and stores Yi script images  \n",
        "- Retrieves a ZIP dataset from source.\n",
        "- Extracts one PNG file per subfolder."
      ],
      "metadata": {
        "id": "S0PDoK-RRGOM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "doQztonnYs4B"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')\n",
        "if not os.path.exists('/content/drive/MyDrive'):\n",
        "    print(\"ERROR: Google Drive was not properly mounted!\")\n",
        "    exit(1)\n",
        "else:\n",
        "    print(\"Google Drive successfully mounted\")\n",
        "\n",
        "yi_dir = \"/content/drive/MyDrive/script_analysis/yi/\"\n",
        "try:\n",
        "    os.makedirs(yi_dir, exist_ok=True)\n",
        "    print(f\"Directory created or already exists at: {yi_dir}\")\n",
        "except Exception as e:\n",
        "    print(f\"ERROR creating directory: {e}\")\n",
        "    exit(1)\n",
        "\n",
        "url = \"https://download.scidb.cn/download?fileId=8f5feddd6987cf95a887c80e6de44c0a&path=/V2/data.zip&fileName=data.zip\"\n",
        "\n",
        "print(\"Downloading ZIP file...\")\n",
        "try:\n",
        "    response = requests.get(url, stream=True)\n",
        "    response.raise_for_status()\n",
        "    print(\"Download successful\")\n",
        "except Exception as e:\n",
        "    print(f\"ERROR downloading file: {e}\")\n",
        "    exit(1)\n",
        "\n",
        "\n",
        "try:\n",
        "    zip_file = zipfile.ZipFile(BytesIO(response.content))\n",
        "\n",
        "    all_files = zip_file.namelist()\n",
        "    print(f\"ZIP contains {len(all_files)} files\")\n",
        "    print(f\"First 5 files: {all_files[:5] if all_files else 'No files found'}\")\n",
        "\n",
        "    print(\"Extracting one PNG per subfolder...\")\n",
        "    subfolder_files = {}\n",
        "\n",
        "    for file in all_files:\n",
        "        if file.lower().endswith('.png'):\n",
        "            folder = os.path.dirname(file)\n",
        "            if folder not in subfolder_files:\n",
        "                subfolder_files[folder] = []\n",
        "            subfolder_files[folder].append(file)\n",
        "\n",
        "    print(f\"Found {len(subfolder_files)} folders with PNG files\")\n",
        "\n",
        "    selected_count = 0\n",
        "    for folder, files in subfolder_files.items():\n",
        "        chosen_file = random.choice(files)\n",
        "        file_path = os.path.join(yi_dir, os.path.basename(chosen_file))\n",
        "\n",
        "        try:\n",
        "            with zip_file.open(chosen_file) as source, open(file_path, \"wb\") as target:\n",
        "                target.write(source.read())\n",
        "            selected_count += 1\n",
        "            print(f\"Saved: {file_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR saving file {chosen_file}: {e}\")\n",
        "\n",
        "    print(f\"Extraction complete! {selected_count} images saved in: {yi_dir}\")\n",
        "\n",
        "    saved_files = os.listdir(yi_dir)\n",
        "    print(f\"Files in destination directory: {len(saved_files)}\")\n",
        "    if saved_files:\n",
        "        print(f\"First few files: {saved_files[:5]}\")\n",
        "    else:\n",
        "        print(\"No files found in destination directory!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"ERROR processing ZIP file: {e}\")\n",
        "    exit(1)\n",
        "\n",
        "print(\"Syncing files to Google Drive...\")\n",
        "!cp /dev/null /content/drive/MyDrive/__force_sync__\n",
        "print(\"Sync complete. Files should now be visible in your Google Drive.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlFl2EvZXr3H"
      },
      "source": [
        "### TYC Data Merging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMX9rZkacZED"
      },
      "source": [
        "Sets up TYC directories, loads image paths, and counts script dataset images.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7_U_EpfX3H2"
      },
      "outputs": [],
      "source": [
        "YI_PATH = '/content/drive/MyDrive/script_analysis/yi/'\n",
        "BA_SHU_PATH = '/content/drive/MyDrive/script_analysis/ba-shu/'\n",
        "OLD_NAXI_PATH = '/content/drive/MyDrive/script_analysis/old_naxi/'\n",
        "TYC_PATH = '/content/drive/MyDrive/script_analysis/tyc/'\n",
        "\n",
        "def setup_directories():\n",
        "    \"\"\"Create the necessary directory structure for the TYC dataset\"\"\"\n",
        "    os.makedirs(TYC_PATH, exist_ok=True)\n",
        "    os.makedirs(os.path.join(TYC_PATH, 'yi'), exist_ok=True)\n",
        "    os.makedirs(os.path.join(TYC_PATH, 'ba-shu'), exist_ok=True)\n",
        "    os.makedirs(os.path.join(TYC_PATH, 'old_naxi'), exist_ok=True)\n",
        "    print(f\"Created TYC directory structure at {TYC_PATH}\")\n",
        "\n",
        "\n",
        "def load_image_paths(directory):\n",
        "    \"\"\"Loads all valid image paths from a directory\"\"\"\n",
        "    image_paths = []\n",
        "    valid_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tif', '.tiff']\n",
        "\n",
        "    if not os.path.exists(directory):\n",
        "        print(f\"Warning: Directory {directory} does not exist!\")\n",
        "        return image_paths\n",
        "\n",
        "    for root, _, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            if any(file.lower().endswith(ext) for ext in valid_extensions):\n",
        "                image_paths.append(os.path.join(root, file))\n",
        "\n",
        "    return image_paths\n",
        "\n",
        "setup_directories()\n",
        "\n",
        "yi_files = load_image_paths(YI_PATH)\n",
        "ba_shu_files = load_image_paths(BA_SHU_PATH)\n",
        "old_naxi_files = load_image_paths(OLD_NAXI_PATH)\n",
        "\n",
        "print(f\"Original dataset counts:\")\n",
        "print(f\"Yi: {len(yi_files)} images\")\n",
        "print(f\"Ba-shu: {len(ba_shu_files)} images\")\n",
        "print(f\"Old Naxi: {len(old_naxi_files)} images\")\n",
        "print(f\"Target count per script: ~{len(yi_files)} images\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eo-94gfUcfaZ"
      },
      "source": [
        "Processing Classical Yi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ja_9a-Pwc--D"
      },
      "outputs": [],
      "source": [
        "def process_yi_dataset():\n",
        "    \"\"\"\n",
        "    Copy Yi dataset files to the TYC structure.\n",
        "    No processing needed as these files are already in good condition.\n",
        "    \"\"\"\n",
        "    yi_paths = load_image_paths(YI_PATH)\n",
        "    target_dir = os.path.join(TYC_PATH, 'yi')\n",
        "\n",
        "    print(f\"Copying {len(yi_paths)} Yi images to {target_dir}...\")\n",
        "\n",
        "    for i, path in enumerate(tqdm(yi_paths)):\n",
        "        filename = os.path.basename(path)\n",
        "\n",
        "        # ensuring unique filenames\n",
        "        target_path = os.path.join(target_dir, f\"yi_{i:05d}_{filename}\")\n",
        "\n",
        "        shutil.copy2(path, target_path)\n",
        "\n",
        "    print(f\"Yi processing complete. {len(yi_paths)} images copied.\")\n",
        "    return len(yi_paths)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UP7QeLkuc-ik"
      },
      "source": [
        "Processing Old Naxi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1Y4aMeXdDN7"
      },
      "outputs": [],
      "source": [
        "def load_image_paths(directory):\n",
        "    \"\"\"Loads all valid image paths from a directory\"\"\"\n",
        "    image_paths = []\n",
        "    valid_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tif', '.tiff']\n",
        "\n",
        "    if not os.path.exists(directory):\n",
        "        print(f\"Warning: Directory {directory} does not exist!\")\n",
        "        return image_paths\n",
        "\n",
        "    for root, _, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            if any(file.lower().endswith(ext) for ext in valid_extensions):\n",
        "                image_paths.append(os.path.join(root, file))\n",
        "\n",
        "    return image_paths\n",
        "\n",
        "def natural_variation(image):\n",
        "    \"\"\"\n",
        "    Apply extremely subtle transformations that preserve the style\n",
        "    and appearance of the original data\n",
        "    \"\"\"\n",
        "    img = image.copy()\n",
        "\n",
        "    # rotation\n",
        "    angle = random.uniform(-2, 2)\n",
        "    img = img.rotate(angle, resample=Image.BICUBIC, expand=False, fillcolor=(255, 255, 255))\n",
        "\n",
        "    # shift\n",
        "    width, height = img.size\n",
        "    shift_x = random.uniform(-width * 0.01, width * 0.01)\n",
        "    shift_y = random.uniform(-height * 0.01, height * 0.01)\n",
        "\n",
        "    # scale\n",
        "    scale = random.uniform(0.98, 1.02)\n",
        "\n",
        "    # tiny affine\n",
        "    img = TF.affine(\n",
        "        img,\n",
        "        angle=0,\n",
        "        translate=(shift_x, shift_y),\n",
        "        scale=scale,\n",
        "        shear=0,\n",
        "        fill=(255, 255, 255)\n",
        "    )\n",
        "\n",
        "    # brightness\n",
        "    if random.random() < 0.3:\n",
        "        factor = random.uniform(0.97, 1.03)\n",
        "        enhancer = ImageEnhance.Brightness(img)\n",
        "        img = enhancer.enhance(factor)\n",
        "\n",
        "    return img\n",
        "\n",
        "def process_old_naxi_dataset(target_count=None):\n",
        "    \"\"\"\n",
        "    Process Old Naxi dataset files:\n",
        "    1. Copy to the TYC structure\n",
        "    2. Add natural variations to reach target count if needed\n",
        "    \"\"\"\n",
        "    old_naxi_paths = load_image_paths(OLD_NAXI_PATH)\n",
        "    target_dir = os.path.join(TYC_PATH, 'old_naxi')\n",
        "\n",
        "    os.makedirs(target_dir, exist_ok=True)\n",
        "\n",
        "    if target_count is None:\n",
        "        yi_files = load_image_paths(YI_PATH)\n",
        "        target_count = len(yi_files)\n",
        "\n",
        "    print(f\"Processing {len(old_naxi_paths)} Old Naxi images...\")\n",
        "\n",
        "    processed_files = []\n",
        "    for i, path in enumerate(tqdm(old_naxi_paths)):\n",
        "        try:\n",
        "            img = Image.open(path).convert('RGB')\n",
        "            filename = os.path.basename(path)\n",
        "            target_path = os.path.join(target_dir, f\"old_naxi_orig_{i:05d}_{filename}\")\n",
        "            img.save(target_path)\n",
        "            processed_files.append(target_path)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {path}: {e}\")\n",
        "\n",
        "\n",
        "    num_variations = max(0, target_count - len(processed_files))\n",
        "\n",
        "    if num_variations > 0:\n",
        "        print(f\"Need to generate {num_variations} additional Old Naxi variations to reach target count\")\n",
        "        create_variations(processed_files, target_dir, num_variations, prefix=\"old_naxi_var\")\n",
        "\n",
        "    final_files = load_image_paths(target_dir)\n",
        "    print(f\"Old Naxi processing complete. {len(final_files)} images processed/generated.\")\n",
        "    return len(final_files)\n",
        "\n",
        "def create_variations(image_paths, target_dir, num_to_generate, prefix=\"var\"):\n",
        "    \"\"\"Create natural variations of the provided images\"\"\"\n",
        "    source_images = []\n",
        "    for path in image_paths:\n",
        "        try:\n",
        "            img = Image.open(path).convert('RGB')\n",
        "            source_images.append(img)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {path}: {e}\")\n",
        "\n",
        "    if not source_images:\n",
        "        print(\"No images to create variations from!\")\n",
        "        return 0\n",
        "\n",
        "    print(f\"Generating {num_to_generate} natural variations...\")\n",
        "    for i in tqdm(range(num_to_generate)):\n",
        "\n",
        "        base_img = random.choice(source_images).copy()\n",
        "\n",
        "        varied_img = natural_variation(base_img)\n",
        "\n",
        "        save_path = os.path.join(target_dir, f\"{prefix}_{i:05d}.png\")\n",
        "        varied_img.save(save_path)\n",
        "\n",
        "    return num_to_generate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_67BWyF5dHuf"
      },
      "source": [
        "Processing Ba-Shu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IiN2GXt7dPZz"
      },
      "outputs": [],
      "source": [
        "def load_image_paths(directory):\n",
        "    \"\"\"Loads all valid image paths from a directory\"\"\"\n",
        "    image_paths = []\n",
        "    valid_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tif', '.tiff']\n",
        "\n",
        "    if not os.path.exists(directory):\n",
        "        print(f\"Warning: Directory {directory} does not exist!\")\n",
        "        return image_paths\n",
        "\n",
        "    for root, _, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            if any(file.lower().endswith(ext) for ext in valid_extensions):\n",
        "                image_paths.append(os.path.join(root, file))\n",
        "\n",
        "    return image_paths\n",
        "\n",
        "def natural_variation(image):\n",
        "    \"\"\"\n",
        "    Apply extremely subtle transformations that preserve the style\n",
        "    and appearance of the original data\n",
        "    \"\"\"\n",
        "    img = image.copy()\n",
        "\n",
        "    # rotation\n",
        "    angle = random.uniform(-2, 2)\n",
        "    img = img.rotate(angle, resample=Image.BICUBIC, expand=False, fillcolor=(255, 255, 255))\n",
        "\n",
        "    # shift\n",
        "    width, height = img.size\n",
        "    shift_x = random.uniform(-width * 0.01, width * 0.01)\n",
        "    shift_y = random.uniform(-height * 0.01, height * 0.01)\n",
        "\n",
        "    # scale\n",
        "    scale = random.uniform(0.98, 1.02)\n",
        "\n",
        "    # tiny affine\n",
        "    img = TF.affine(\n",
        "        img,\n",
        "        angle=0,\n",
        "        translate=(shift_x, shift_y),\n",
        "        scale=scale,\n",
        "        shear=0,\n",
        "        fill=(255, 255, 255)\n",
        "    )\n",
        "\n",
        "    # brightness\n",
        "    if random.random() < 0.3:\n",
        "        factor = random.uniform(0.97, 1.03)\n",
        "        enhancer = ImageEnhance.Brightness(img)\n",
        "        img = enhancer.enhance(factor)\n",
        "\n",
        "    return img\n",
        "\n",
        "def process_ba_shu_dataset(target_count=None):\n",
        "    \"\"\"Process Ba-shu dataset with subtle natural variations\"\"\"\n",
        "    ba_shu_paths = load_image_paths(BA_SHU_PATH)\n",
        "    target_dir = os.path.join(TYC_PATH, 'ba-shu')\n",
        "\n",
        "    os.makedirs(target_dir, exist_ok=True)\n",
        "\n",
        "    if target_count is None:\n",
        "        yi_files = load_image_paths(YI_PATH)\n",
        "        target_count = len(yi_files)\n",
        "\n",
        "    print(f\"Copying {len(ba_shu_paths)} original Ba-shu images...\")\n",
        "\n",
        "    for i, path in enumerate(tqdm(ba_shu_paths)):\n",
        "        filename = os.path.basename(path)\n",
        "        target_path = os.path.join(target_dir, f\"ba_shu_orig_{i:05d}_{filename}\")\n",
        "        shutil.copy2(path, target_path)\n",
        "\n",
        "    num_variations = max(0, target_count - len(ba_shu_paths))\n",
        "\n",
        "    if num_variations == 0:\n",
        "        print(\"No need for additional variations, Ba-shu dataset has enough samples.\")\n",
        "        return len(ba_shu_paths)\n",
        "\n",
        "    print(f\"Generating {num_variations} additional Ba-shu variations...\")\n",
        "\n",
        "    original_images = []\n",
        "    for path in ba_shu_paths:\n",
        "        try:\n",
        "            img = Image.open(path).convert('RGB')\n",
        "            original_images.append(img)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {path}: {e}\")\n",
        "\n",
        "    if not original_images:\n",
        "        print(\"No images could be loaded for variations\")\n",
        "        return len(ba_shu_paths)\n",
        "\n",
        "    print(f\"Successfully loaded {len(original_images)} images for creating variations\")\n",
        "\n",
        "    batch_size = min(100, num_variations)\n",
        "\n",
        "    total_generated = 0\n",
        "    rounds = 0\n",
        "\n",
        "    # generate variations in rounds\n",
        "    while total_generated < num_variations:\n",
        "        rounds += 1\n",
        "        print(f\"Starting variation round {rounds}...\")\n",
        "\n",
        "        to_generate = min(batch_size, num_variations - total_generated)\n",
        "\n",
        "        for i in tqdm(range(to_generate)):\n",
        "            base_img = random.choice(original_images).copy()\n",
        "\n",
        "            varied = natural_variation(base_img)\n",
        "\n",
        "            save_path = os.path.join(target_dir, f\"ba_shu_var_{total_generated:05d}.png\")\n",
        "            varied.save(save_path)\n",
        "            total_generated += 1\n",
        "\n",
        "    total_images = len(ba_shu_paths) + total_generated\n",
        "    print(f\"Ba-shu processing complete. {total_images} total images created.\")\n",
        "    return total_images\n",
        "\n",
        "def show_ba_shu_samples():\n",
        "    ba_shu_dir = os.path.join(TYC_PATH, 'ba-shu')\n",
        "    if not os.path.exists(ba_shu_dir):\n",
        "        print(\"Ba-shu directory not found\")\n",
        "        return\n",
        "\n",
        "    files = load_image_paths(ba_shu_dir)\n",
        "    if not files:\n",
        "        print(\"No Ba-shu files found\")\n",
        "        return\n",
        "\n",
        "    orig_files = [f for f in files if 'orig' in f]\n",
        "    var_files = [f for f in files if 'var' in f]\n",
        "\n",
        "    if not var_files:\n",
        "        var_files = [f for f in files if 'aug' in f]\n",
        "\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    num_samples = min(4, len(orig_files))\n",
        "    num_var_samples = min(8, len(var_files))\n",
        "\n",
        "    if orig_files:\n",
        "        for i, path in enumerate(random.sample(orig_files, num_samples)):\n",
        "            img = Image.open(path).convert('RGB')\n",
        "            plt.subplot(3, 4, i+1)\n",
        "            plt.imshow(img)\n",
        "            plt.title(f\"Original {i+1}\")\n",
        "            plt.axis('off')\n",
        "\n",
        "    if var_files:\n",
        "        for i, path in enumerate(random.sample(var_files, num_var_samples)):\n",
        "            img = Image.open(path).convert('RGB')\n",
        "            plt.subplot(3, 4, i+num_samples+1)\n",
        "            plt.imshow(img)\n",
        "            plt.title(f\"Variation {i+1}\")\n",
        "            plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3CEebdCdWaB"
      },
      "source": [
        "Processes and visualizes the combined TYC dataset  \n",
        "- Standardizes, augments, and balances Yi, Ba-Shu, and Old Naxi datasets.  \n",
        "- Generates a summary of dataset sizes and saves processed images.  \n",
        "- Displays sample images from each script category.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1kW92AadXum"
      },
      "outputs": [],
      "source": [
        "def process_all_datasets():\n",
        "    \"\"\"Process all datasets and create the combined TYC dataset\"\"\"\n",
        "    print(\"Processing Yi dataset...\")\n",
        "    yi_count = process_yi_dataset()\n",
        "\n",
        "    print(\"\\nProcessing Old Naxi dataset...\")\n",
        "    old_naxi_count = process_old_naxi_dataset()\n",
        "\n",
        "    print(\"\\nProcessing Ba-shu dataset...\")\n",
        "    ba_shu_count = process_ba_shu_dataset(yi_count)\n",
        "\n",
        "    # final counts\n",
        "    final_yi_count = len(load_image_paths(os.path.join(TYC_PATH, 'yi')))\n",
        "    final_ba_shu_count = len(load_image_paths(os.path.join(TYC_PATH, 'ba-shu')))\n",
        "    final_old_naxi_count = len(load_image_paths(os.path.join(TYC_PATH, 'old_naxi')))\n",
        "\n",
        "    print(\"\\n=== TYC Dataset Summary ===\")\n",
        "    print(f\"Yi: {final_yi_count} images\")\n",
        "    print(f\"Ba-shu: {final_ba_shu_count} images\")\n",
        "    print(f\"Old Naxi: {final_old_naxi_count} images\")\n",
        "    print(f\"Total: {final_yi_count + final_ba_shu_count + final_old_naxi_count} images\")\n",
        "    print(f\"Dataset saved to: {TYC_PATH}\")\n",
        "\n",
        "    show_samples()\n",
        "\n",
        "def show_samples():\n",
        "    \"\"\"Show sample images from each processed dataset\"\"\"\n",
        "    plt.figure(figsize=(15, 8))\n",
        "\n",
        "    # yi samples\n",
        "    yi_files = load_image_paths(os.path.join(TYC_PATH, 'yi'))\n",
        "    if yi_files:\n",
        "        for i, path in enumerate(random.sample(yi_files, min(3, len(yi_files)))):\n",
        "            img = Image.open(path).convert('RGB')\n",
        "            plt.subplot(3, 3, i+1)\n",
        "            plt.imshow(img)\n",
        "            plt.title(f\"Yi {i+1}\")\n",
        "            plt.axis('off')\n",
        "\n",
        "    # ba-shu samples\n",
        "    ba_shu_files = load_image_paths(os.path.join(TYC_PATH, 'ba-shu'))\n",
        "    if ba_shu_files:\n",
        "        for i, path in enumerate(random.sample(ba_shu_files, min(3, len(ba_shu_files)))):\n",
        "            img = Image.open(path).convert('RGB')\n",
        "            plt.subplot(3, 3, i+4)\n",
        "            plt.imshow(img)\n",
        "            plt.title(f\"Ba-shu {i+1}\")\n",
        "            plt.axis('off')\n",
        "\n",
        "    # old naxi samples\n",
        "    old_naxi_files = load_image_paths(os.path.join(TYC_PATH, 'old_naxi'))\n",
        "    if old_naxi_files:\n",
        "        for i, path in enumerate(random.sample(old_naxi_files, min(3, len(old_naxi_files)))):\n",
        "            img = Image.open(path).convert('RGB')\n",
        "            plt.subplot(3, 3, i+7)\n",
        "            plt.imshow(img)\n",
        "            plt.title(f\"Old Naxi {i+1}\")\n",
        "            plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "process_all_datasets()"
      ]
    }
  ]
}